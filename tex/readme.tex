\documentclass[11pt]{article}
\usepackage[in]{fullpage}

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{listings}
\usepackage{color}
\usepackage{amsmath}
\usepackage{cancel}
\usepackage{amssymb}
\usepackage{booktabs}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\newcommand{\qed}{\hfill\ensuremath{\blacksquare}}
\newcommand{\events}{\mathcal{E}}

\lstset{
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\title{601.665 --- Natural Language Processing\\Assignment 3: Probabilities and Vectors}
\author{Aaron Mueller}
\date{26 September 2018}

\begin{document}
\maketitle

1. The per-word cross-entropy of a language model with add-0.01 smoothing built from
\texttt{switchboard-small} on the \texttt{sample} test files are as follows:
\begin{align*}
H(p,q) = -\frac{1}{m}\sum_{i=0}^m p(w_i)\log(q(w_i))
\end{align*}
Where $p$ is the language model and $q$ is the \texttt{sample} file. We already have the
sum of the log probabilities per word, calculated by \texttt{fileprob.py};
thus, we can calculate $H$ for all the sample files:
\begin{align*}
H(p,\texttt{sample1}) &= -\frac{1}{m}\cdot-12121\\
&= \frac{12121}{1686} \approx 7.189\\
H(p,\texttt{sample2}) &= -\frac{1}{m}\cdot-7398.55\\
&= \frac{7398.55}{978} \approx 7.565\\
H(p,\texttt{sample3}) &= -\frac{1}{m}\cdot-7477.99\\
&= -\frac{7477.99}{985} \approx 7.592
\end{align*}
Then, for the per-word perplexity, we exponentiate 2 to the power of these cross-entropies:
\begin{align*}
perplexity(p,\texttt{sample1}) = 2^{7.189} = 145.9\\
perplexity(p,\texttt{sample2}) = 2^{7.565} = 189.4\\
perplexity(p,\texttt{sample3}) = 2^{7.592} = 192.9\\
\end{align*}

When we build our language model instead on the larger \texttt{switchboard} corpus, we have
slightly lower $\log_2$ probabilities (i.e., negative $\log_2$ probabilities with
greater magnitude), which thus leads to higher cross-entropies and higher perplexities
s well. This is because our language model built on the larger corpus has seen a greater
number of word types, which thus slightly reduces the probability of seeing any given word
type. Since we have these smaller probabilities, our $\log_2$ probabilities on the
\texttt{sample} files will be somewhat more negative since our summation will see smaller
probabilities that are put through a logarithmic function. Because of these more
negative $\log_2$ probabilities, we also see greater cross-entropies since $m$, the
number of words in the file, stays the same, while the numerator increases in magnitude.
This higher cross-entropy then leads to higher perplexities upon exponentiation.
\pagebreak

2. 

\end{document}