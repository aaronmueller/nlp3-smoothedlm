A version of this Python port for an earlier version of this
assignment was kindly provided by Eric Perlman, a previous student in
the NLP class.  Thanks to Prof. Jason Baldridge at U. of Texas for
updating the code and these instructions when he borrowed the
assignment.  They were subsequently modified by Xuchen Yao, Mozhi
Zhang and Chu-Cheng Lin for more recent versions of the assignment.

----------------------------------------------------------------------

Copy these code files to a private directory of yours, DIR.  
You can do this as follows:

  mkdir MYDIR
  cp -p /usr/local/data/cs465/hw-lm/code/python/* MYDIR
  cd MYDIR

----------

QUESTION 1.
Type "./fileprob.py" without arguments to see documentation.  
Try the examples mentioned in the documentation.

----------

QUESTION 2.

Copy fileprob.py to textcat.py.

Modify textcat.py so that it does text categorization.

For each training corpus, you should create a new language model.  You
will first need to call set_vocab_size() on the pair of corpora, so
that both models use the same vocabulary (derived from the union of
the two corpora).  Note that set_vocab_size can take multiple files as
arguments.  You can re-use the current LanguageModel object by using the
following strategy.

  (In TRAIN mode)
  call lm.setVocabSize() on the pair of training corpora

  train model 1 from corpus 1
  train model 2 from corpus 2
  store the model parameters
  terminate program

  ===

  (In TEST mode)
  restore model 1 and model 2 the previously saved parameters
  for each file,
    compute its probability under model 1: save this in an array

  for each file,
    compute its probability under model 2: save this in an array
  loop over the arrays and print the results

There are other ways to solve this problem, if you prefer. However we
require your TextCat to have two modes TRAIN and TEST. The TRAIN mode
should train the models and save the parameters. The TEST mode should
load the previously saved parameters and compute/print the results
without looking at corpus 1 and 2.

----------

QUESTION 5.

Modify the prob() function in Probs.py.  You are just filling in the
case BACKOFF_ADDL.

Remember you need to handle OOV words, and make sure the probabilities
of all possible words after a given context sums up to one.

As you are only adding a new model, the behavior of your old models such
as ADDL should not change.

----------------------------------------------------------------------

QUESTION 6.

(a) Modify the prob() function in Probs.py.  You are just filling in the
case LOGLINEAR.

Remember you need to handle *OOL* words.  This is slightly different than
handling OOV words.

(b) Implement stochastic gradient ascent in the train() function in Probs.py.

(e) Modify both prob() and train() to add the new feature.

As you are only adding a new model, nothing that you do should change
your previous results.

Using vector/matrix operation (optional):
Training the log-linear model on en.1K can be done with simple "for" loops and
2D array representation of matrices.  However, you're welcome to use NumPy's
matrix/vector operations, which might reduce training time and simplify your
code.

----------

Extra Credit Question 8.
Copy fileprob to speechrec.
Modify speechrec so that it does text categorization.
You only have one training corpus now, making things easier.

----------

Extra Credit Question 10.  

Modify the prob() function in Probs.py.  You will fill in the case
BACKOFF_WB.

You'll also need to modify other parts of Probs.pm -- in particular,
adding code to the train() subroutine and the global variable
declarations.

Again, as you are only adding a new model, nothing that you do should
change your previous results.

----------
